# çª®é€¼ã®è‡ªç„¶èªè¨€æ¨¡çµ„

## ç’°å¢ƒé…ç½®

- pyenv install 3.8.20
- pip install "hanlp[full]"
- pip install -r requirements.txt

<!--
# MAC Mç³»åˆ—å•Ÿç”¨GPU
pip install tensorflow-macos
pip install tensorflow-metal

import tensorflow as tf

print("TensorFlow ç‰ˆæœ¬:", tf.__version__)
print("å¯ç”¨ GPU æ•°:", len(tf.config.list_physical_devices('GPU')))
print("è®¾å¤‡åˆ—è¡¨:", tf.config.list_physical_devices())
-->

## ç›®éŒ„çµæ§‹ç”Ÿæˆ

- mkdir data
- touch data/trian.txt (80%æ•¸æ“š)
- touch data/dev.txt (20%æ•¸æ“š)
  -->

## å·¥ä½œæµä»‹ç´¹ (HANLP2)

- TOKEN (åˆ†å‰²å¥å­æˆå–®ä½è©)

1. åœ¨ç·šæ¨¡çµ„: https://hanlp.hankcs.com/docs/api/hanlp/pretrained/tok.html
2. å°ˆæ¡ˆæ¨¡çµ„: hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH
3. è‡ªå®šç¾©å­—å…¸

- POS (è©æ€§)

1. åœ¨ç·šæ¨¡çµ„: https://hanlp.hankcs.com/docs/api/hanlp/pretrained/pos.html
2. å°ˆæ¡ˆæ¨¡çµ„: hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL
3. è‡ªå®šç¾©å‘½åè¦å‰‡

- NER (å–®ä½è©çš„å‘½å)

1. åœ¨ç·šæ¨¡çµ„: https://hanlp.hankcs.com/docs/api/hanlp/pretrained/ner.html
2. å°ˆæ¡ˆæ¨¡çµ„: transformer='bert-base-chinese'
3. è‡ªå®šç¾©å‘½åè¦å‰‡

- SRL (èªæ„è§’è‰²)

1. åœ¨ç·šæ¨¡çµ„: https://hanlp.hankcs.com/docs/api/hanlp/pretrained/srl.html
2. å°ˆæ¡ˆæ¨¡çµ„: hanlp.pretrained.srl.CPB3_SRL_ELECTRA_SMALL

## æº–å‚™ææ–™ (BIOè¦ç¯„)

- è¨“ç·´è³‡æ–™ (data/train.txt)

```txt
æˆ‘    O
æƒ³    O
æŸ¥    O
åš    B-PRODUCT
å£«    I-PRODUCT
ä¼¦    I-PRODUCT
-    I-PRODUCT
X    I-PRODUCT
Y    I-PRODUCT
9    I-PRODUCT
9    I-PRODUCT
åº“    O
å­˜    O
```

- é©—è­‰è³‡æ–™ (data/dev.txt)

```txt
æŸ¥ O
è¯¢ O
ç´… B-PRODUCT
è˜‹ I-PRODUCT
æœ I-PRODUCT
åº“ O
å­˜ O
```

## è®­ç»ƒHANLP2

```python
import hanlp
import os

"""
# åŠ é€Ÿ (INTEL)
pip install --upgrade intel-tensorflow
"""
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªé¡¯ç¤º Error
from hanlp.components.ner.transformer_ner import TransformerNamedEntityRecognizer

recognizer = TransformerNamedEntityRecognizer()
save_dir = 'data/model/ner/product_bert'

recognizer.fit(
    trn_data='data/train.txt',
    dev_data='data/dev.txt',
    save_dir=save_dir,
    transformer='bert-base-chinese',
    # bert-base-chinese æˆ– electra-small (CPUæ…¢), å¯é¸: 1.FINE_ELECTRA_SMALL_ZH MSRA_NER_ELECTRA_SMALL_ZH
    epochs=10,
    batch_size=8,
    word_dropout=0.05,
    delimiter_in_entity='-',
    char_level=True  # ğŸ‘ˆ åŠ é€™å€‹
)

recognizer = hanlp.load(save_dir)
test_sentences = 'æŸ¥è¯¢åšå£«ä¼¦-XY99åº“å­˜'
results = recognizer.predict(test_sentences)
print(results)
```

## æµ‹è¯•

```
import hanlp

# è¼‰å…¥ä½ è¨“ç·´å¥½çš„æ¨¡å‹
ner = hanlp.load('product_ner_model')

text = "æŸ¥è¯¢æµ·ä¿ªæ©-120013-L000P3 åå…‰åº“å­˜"
result = ner(text)

print(result)
```

## æ¸¬è©¦[2]

```python
# 1ï¸âƒ£ åŠ è½½æ¨¡å‹
import hanlp

# 1ï¸âƒ£ åŠ è½½æ¨¡å‹
tokenizer = hanlp.load('FINE_ELECTRA_SMALL_ZH')  # åˆ†è©å™¨
pos_tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN)  # è©æ€§æ¨™è¨»å™¨
ner_model = hanlp.load('data/model/ner/product_bert')  # å‘½åå¯¦é«”è­˜åˆ¥æ¨¡å‹

# 2ï¸âƒ£ æ„å»º pipeline
pipeline = hanlp.pipeline()
.append(tokenizer, output_key='tok')
.append(pos_tagger, input_key='tok', output_key='pos')
.append(ner_model, input_key='tok', output_key='ner')

# 3ï¸âƒ£ æµ‹è¯•å¥å­
sentence = 'æŸ¥è¯¢ME&CITY4æ”¯è£…å±•æ¶ï¼ˆå¤ªé˜³é•œï¼‰åº“å­˜'
res = pipeline(sentence)

# 4ï¸âƒ£ è¾“å‡ºæ¯ä¸ªè¯çš„åˆ†è¯ã€è¯æ€§ã€NER
for token, pos, ner in zip(res['tok'], res['pos'], res['ner']):
    print(token, pos, ner)

```

## å•Ÿå‹•API

```shell
gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.main:app --bind 0.0.0.0:8000
# uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

## é–‹æ©Ÿè‡ªå‹•å•Ÿå‹•

```shell
sudo systemctl daemon-reload
sudo systemctl enable hanlp-api.service  # é–‹æ©Ÿè‡ªå‹•å•Ÿå‹•
sudo systemctl start hanlp-api.service   # ç«‹å³å•Ÿå‹•
```

## è‡ªå‹•æ–‡ä»¶

```shell
cat << EFO > /etc/systemd/system/hanlp-api.service
[Unit]
Description=HanLP FastAPI Service
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/Hanlp2
Environment="PATH=/opt/Hanlp2/.venv/bin:/usr/bin:/bin"
ExecStart=/root/.pyenv/versions/3.8.20/bin/gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.main:app --bind 0.0.0.0:8000
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EFO
```

<!--
# è¨ªå•è·¯å¾‘
http://127.0.0.1:8000/docs
-->

## ç›®éŒ„çµæ§‹

```text
Hanlp2/
â”œâ”€â”€ app/                  # API èˆ‡ç¨‹å¼ç¢¼
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py           # FastAPI API
â”‚   â”œâ”€â”€ pipeline.py       # NLP pipeline å°è£
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•¸
â”œâ”€â”€ data/                 # è¨“ç·´è³‡æ–™
â”‚   â”œâ”€â”€ train.txt
â”‚   â”œâ”€â”€ dev.txt
â”‚   â””â”€â”€ model/ner/product_bert/
â”œâ”€â”€ scripts/              # è¨“ç·´ã€æ¸¬è©¦è…³æœ¬
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ test.py
â”œâ”€â”€ tests/                # å–®å…ƒæ¸¬è©¦
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ requirements.txt      # å¥—ä»¶ä¾è³´
â””â”€â”€ README.md
```

## è¨“ç·´é‡é»

- B-NR è·Ÿ E-NR ä¹‹é–“çš„I-NRå…§å®¹æ ¼å¼éœ€è¦å¤šå…ƒ

## æ–‡ä»¶

- https://hanlp.hankcs.com/docs/api/hanlp/pretrained/ner.html
- https://hanlp.hankcs.com/demos/tok.html
- https://hanlp.hankcs.com/docs/annotations/pos/ctb.html

## æœ€ä½³çµ„åˆ

- ä»£ç¢¼é¡ â†’ tokenizer.dict_force (ç©©å®šã€æ ¼å¼å›ºå®š)
- åç¨±é¡ â†’ NERï¼ˆDictionaryNER / è¨“ç·´æ¨¡å‹ï¼‰ï¼ˆä¸è¦å‰‡ã€è®Šå‹•å¤šï¼‰

## è‡ªå®šç¾©è¾­å…¸

BD = Business Documentï¼ˆç¼©å†™ï¼‰

## è¦å‰‡æ–¹æ¡ˆ

- æ¨è–¦ (éƒ¨åˆ†è¼¸å…¥è©: é¦®å‡±, å¯ä»¥åŒ¹é…)

1. å­—å…¸åŠ å…¥å…¨åï¼š æ¥Šé¦®å‡±
2. NERè¨“ç·´é›†éƒ¨åˆ†åï¼š

```tsv
é¦® B-AGENT
å‡± I-AGENT
```

- ä¸æ¨è–¦ (éƒ¨åˆ†è¼¸å…¥è©: é¦®å‡±, ç„¡æ³•åŒ¹é…)

1. å­—å…¸åŠ å…¥å…¨åï¼š æ¥Šé¦®å‡±
2. NERè¨“ç·´é›†å…¨åï¼š

```tsv
æ¥Š B-AGENT
é¦® I-AGENT
å‡± I-AGENT
```

# è¨“ç·´é©—è­‰
1. è¨“ç·´é›†lossä»ä¸‹é™ â†’ æ¨¡å‹å°è¨“ç·´è³‡æ–™æ“¬åˆèƒ½åŠ›é‚„åœ¨æå‡
2. é©—è­‰é›† F1 å·²é”æ»¿åˆ† â†’ æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šå·²ç¶“æ²’æœ‰æå‡ç©ºé–“
3. æ„å‘³è‘—, å†å¤šè·‘å¹¾å€‹ epochï¼Œé©—è­‰é›†æ€§èƒ½ä¸æœƒæå‡, ç¹¼çºŒè¨“ç·´å¯èƒ½å°è‡´éæ“¬åˆ
4. éæ“¬åˆï¼ˆOverfittingï¼‰æ˜¯æŒ‡æ¨¡å‹ã€Œè¨˜ä½è¨“ç·´è³‡æ–™è€Œä¸æ˜¯å­¸ç¿’è¦å¾‹ã€ã€‚
```
Epoch 1 / 10:
185/185 loss: 243.9575 P: 49.92% R: 73.29% F1: 59.39% ET: 19 m 31 s
185/185 loss: 0.1209 P: 99.99% R: 99.99% F1: 99.99% ET: 5 m 52 s
25 m 23 s / 4 h 13 m 50 s ETA: 3 h 48 m 27 s (saved)
Epoch 2 / 10:
185/185 loss: 4.0599 P: 69.94% R: 86.40% F1: 77.30% ET: 19 m 26 s
185/185 loss: 0.0554 P: 99.99% R: 99.99% F1: 99.99% ET: 5 m 46 s
50 m 47 s / 4 h 13 m 56 s ETA: 3 h 23 m 9 s (1)
Epoch 3 / 10:
185/185 loss: 1.8789 P: 78.52% R: 90.86% F1: 84.24% ET: 19 m 15 s
185/185 loss: 0.0540 P: 99.99% R: 99.99% F1: 99.99% ET: 5 m 46 s
1 h 15 m 48 s / 4 h 12 m 40 s ETA: 2 h 56 m 52 s (2)
Epoch 4 / 10:
185/185 loss: 1.0521 P: 83.28% R: 93.11% F1: 87.92% ET: 19 m 25 s
 58/185 loss: 0.0012 P: 100.00% R: 100.00% F1: 100.00% ETA: 4 m 18 s
```